{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from utils import *\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msetupishe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    USED_CLASSES = [0, 1, 2, 3, 6, 7, 8, 9]\n",
    "\n",
    "    IMAGE_SIZE: tuple[int,int] = (448, 448) # W, H\n",
    "    BACKGROUND_CLS_ID: int = 0\n",
    "    DATASET_PATH: str = 'data/default_dataset'\n",
    "    AUGS = [A.HorizontalFlip(p=0.5), \n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(scale_limit=0.12, rotate_limit=0.15, shift_limit=0.12, p=0.5),\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_mapping(used_classes):\n",
    "    class_mapping = np.zeros(len(labels_dict), dtype=int)\n",
    "    for new_class, original_class in enumerate(used_classes):\n",
    "        class_mapping[original_class] = new_class\n",
    "    return class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = create_class_mapping(DatasetConfig.USED_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 0, 0, 4, 5, 6, 7, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrgansDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                dataset_path: str, \n",
    "                img_size: int,\n",
    "                augs: List | None = None,\n",
    "                cache: bool = False,\n",
    "                clip_min: int | None = None,\n",
    "                clip_max: int | None = None,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.use_cache = cache\n",
    "        self.img_size = img_size\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "\n",
    "        for img_path in glob.glob(dataset_path + '/**/*img.npy', recursive=True):\n",
    "            lbl_path = img2label(img_path)\n",
    "            self.images.append(load_npy(img_path) if self.use_cache else img_path)\n",
    "            self.labels.append(load_npy(img_path) if self.use_cache else lbl_path)\n",
    "\n",
    "        transforms = []\n",
    "        if augs is not None:\n",
    "            transforms.extend(augs)\n",
    "        transforms.extend([\n",
    "                        A.Resize(self.img_size, self.img_size, always_apply=True),\n",
    "                        ToTensorV2(always_apply=True)\n",
    "                    ])\n",
    "        self.transforms = A.Compose(transforms)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if not self.use_cache:\n",
    "            image = load_npy(image)\n",
    "            label = load_npy(label)\n",
    "        \n",
    "        image = normalize(image, \n",
    "                          min_val=self.clip_min,\n",
    "                          max_val=self.clip_max,\n",
    "                          )\n",
    "        image = np.expand_dims(image, 2)\n",
    "        label = class_mapping[label.astype(int)]\n",
    "        transformed = self.transforms(image=image, mask=label)\n",
    "        image, label = transformed[\"image\"], transformed[\"mask\"].to(torch.long)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder (downsampling)\n",
    "        # Each enc_conv/dec_conv block should look like this:\n",
    "        # nn.Sequential(\n",
    "        #     nn.Conv2d(...),\n",
    "        #     ... (2 or 3 conv layers with relu and batchnorm),\n",
    "        # )\n",
    "        self.pooling = F.max_pool2d\n",
    "        self.pool_params = {\"kernel_size\":2, \"stride\":2}\n",
    "        self.enc_conv0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.enc_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.enc_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # decoder (upsampling)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.dec_conv0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3,\n",
    "                      padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=out_channels, kernel_size=3,\n",
    "                      padding = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder                                                  (size, features)\n",
    "\n",
    "        e0 = self.enc_conv0(x) #(256, 32)\n",
    "        e1 = self.enc_conv1(self.pooling(e0, **self.pool_params)) #(128, 64)\n",
    "        e2 = self.enc_conv2(self.pooling(e1, **self.pool_params)) #(64, 128)\n",
    "        e3 = self.enc_conv3(self.pooling(e2, **self.pool_params)) #(32, 256)\n",
    "\n",
    "        b = self.bottleneck_conv(self.pooling(e3, **self.pool_params)) #(16, 256)\n",
    "\n",
    "\n",
    "        d0 = self.dec_conv0(torch.cat((self.upsample(b), e3), 1)) #(32, 128)\n",
    "        d1 = self.dec_conv1(torch.cat((self.upsample(d0), e2), 1)) #(64, 64)\n",
    "        d2 = self.dec_conv2(torch.cat((self.upsample(d1), e1), 1)) #(128, 32)\n",
    "        d3 = self.dec_conv3(torch.cat((self.upsample(d2), e0), 1)) #(256, 1)\n",
    "\n",
    "        return d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(predictions, ground_truths, num_classes=2, dims=(1, 2), smooth=1e-8, ce=False):\n",
    "    \"\"\"Smooth Dice coefficient + Cross-entropy loss function.\"\"\"\n",
    "    ground_truth_oh = F.one_hot(ground_truths, num_classes=num_classes)\n",
    "    prediction_norm = F.softmax(predictions, dim=1).permute(0, 2, 3, 1)\n",
    "    intersection = (prediction_norm * ground_truth_oh).sum(dim=dims)\n",
    "    summation = prediction_norm.sum(dim=dims) + ground_truth_oh.sum(dim=dims)\n",
    "\n",
    "    dice = (2.0 * intersection + smooth) / (summation + smooth)\n",
    "    dice_mean = dice.mean()\n",
    "    loss = 1.0 - dice_mean\n",
    "    if ce:\n",
    "        CE = F.cross_entropy(predictions, ground_truths)\n",
    "        loss += CE\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
       "         [0.5000, 0.6000, 0.7000, 0.8000, 0.9000],\n",
       "         [1.0000, 1.1000, 1.2000, 1.3000, 1.4000]],\n",
       "\n",
       "        [[1.5000, 1.6000, 1.7000, 1.8000, 1.9000],\n",
       "         [2.0000, 2.1000, 2.2000, 2.3000, 2.4000],\n",
       "         [2.5000, 2.6000, 2.7000, 2.8000, 2.9000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(2*3*5).reshape(2, 3, 5)/10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1863, 0.1863, 0.1863, 0.1863, 0.1863],\n",
       "         [0.3072, 0.3072, 0.3072, 0.3072, 0.3072],\n",
       "         [0.5065, 0.5065, 0.5065, 0.5065, 0.5065]],\n",
       "\n",
       "        [[0.1863, 0.1863, 0.1863, 0.1863, 0.1863],\n",
       "         [0.3072, 0.3072, 0.3072, 0.3072, 0.3072],\n",
       "         [0.5065, 0.5065, 0.5065, 0.5065, 0.5065]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(a, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, metric):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0\n",
    "    processed_data = 0\n",
    "\n",
    "    count = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels, num_classes=len(DatasetConfig.USED_CLASSES))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_metric += metric(preds, labels)\n",
    "        processed_data += inputs.size(0)\n",
    "        count += 1\n",
    "\n",
    "    train_loss = running_loss / processed_data\n",
    "    train_metric = running_metric.cpu().numpy() / count\n",
    "    return train_loss, train_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, val_loader, criterion, metric):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    count = 0\n",
    "    for inputs, labels in tqdm(val_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels, num_classes=len(DatasetConfig.USED_CLASSES))\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_metric += metric(preds, labels)\n",
    "        processed_size += inputs.size(0)\n",
    "        count += 1\n",
    "\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_metric = running_metric.double() / count\n",
    "    return val_loss, val_metric.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_loader, val_loader, optim, criterion, metric, learning_rate=0.001):\n",
    "\n",
    "    # best_model_wts = model.state_dict()\n",
    "    # best_metric = 0.0\n",
    "    # best_epoch = 0\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_metric {t_metric:0.4f} val_metric {v_metric:0.4f}\"\n",
    "\n",
    "    \n",
    "    opt = optim(model.parameters(), lr = learning_rate)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min',\n",
    "    #                                                        patience = 3,\n",
    "    #                                                        threshold=0.001,\n",
    "    #                                                        verbose = True,\n",
    "    #                                                        factor  = 0.5)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=4,\n",
    "    #                                                      verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_metric = fit_epoch(model, train_loader, criterion, opt, metric)\n",
    "\n",
    "\n",
    "        val_loss, val_metric = eval_epoch(model, val_loader, criterion, metric)\n",
    "        # scheduler.step(val_loss)\n",
    "        history.append((train_loss, train_metric, val_loss, val_metric))\n",
    "\n",
    "\n",
    "\n",
    "        # if val_metric > best_metric:\n",
    "        #     best_metric = val_metric\n",
    "        #     best_model_wts = model.state_dict()\n",
    "        #     best_epoch = epoch\n",
    "\n",
    "        print(f\"\\nEpoch {epoch:03d} train_loss: {train_loss:0.4f} \\\n",
    "    val_loss {val_loss:0.4f} train_metric {train_metric:0.4f} val_metric {val_metric:0.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(1, len(DatasetConfig.USED_CLASSES)).to(device)\n",
    "metric = MulticlassF1Score(num_classes=len(DatasetConfig.USED_CLASSES), average=\"macro\").to(device)\n",
    "optimiser = optim.Adam\n",
    "criterion = dice_coef_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OrgansDataset('data/train', 224, augs = DatasetConfig.AUGS)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "val_dataset = OrgansDataset('data/val', 224)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 10.69it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 000 train_loss: 0.8125     val_loss 0.7857 train_metric 0.1942 val_metric 0.2430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.15it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 0.7246     val_loss 0.7452 train_metric 0.3221 val_metric 0.3233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.12it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 0.6644     val_loss 0.8004 train_metric 0.4214 val_metric 0.2717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.07it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 26.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 0.6368     val_loss 0.6453 train_metric 0.4679 val_metric 0.4623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.10it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 0.6092     val_loss 0.6168 train_metric 0.5137 val_metric 0.5241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.06it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 0.5954     val_loss 0.6056 train_metric 0.5385 val_metric 0.5398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.18it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 006 train_loss: 0.5808     val_loss 0.6227 train_metric 0.5660 val_metric 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.13it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 007 train_loss: 0.5709     val_loss 0.6078 train_metric 0.5803 val_metric 0.5436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.16it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 008 train_loss: 0.5587     val_loss 0.6010 train_metric 0.6042 val_metric 0.5583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.10it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 009 train_loss: 0.5527     val_loss 0.5575 train_metric 0.6104 val_metric 0.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.04it/s]\n",
      "100%|██████████| 33/33 [00:01<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 010 train_loss: 0.5421     val_loss 0.5936 train_metric 0.6360 val_metric 0.5836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 40/115 [00:03<00:06, 11.01it/s]"
     ]
    }
   ],
   "source": [
    "train(model, 100, train_loader, val_loader, optimiser, criterion, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
